{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "from gym import spaces\n",
    "import math\n",
    "from pygame.math import Vector2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pygame\n",
    "\n",
    "def distance(x1, y1, x2, y2):\n",
    "    dx = x2 - x1\n",
    "    dy = y2 - y1\n",
    "    distance = math.sqrt(dx**2 + dy**2)\n",
    "    return distance\n",
    "\n",
    "def intersection(x1, y1, x2, y2, x3, y3, x4, y4):\n",
    "    line1_start = Vector2(x1, y1)\n",
    "    line1_end = Vector2(x2, y2)\n",
    "    line2_start = Vector2(x3, y3)\n",
    "    line2_end = Vector2(x4, y4)\n",
    "\n",
    "    line1_direction = line1_end - line1_start\n",
    "    line2_direction = line2_end - line2_start\n",
    "\n",
    "    if line1_direction.cross(line2_direction) != 0:\n",
    "        intersection_point = line1_start + line1_direction * ((line2_start - line1_start).cross(line2_direction) / line1_direction.cross(line2_direction))\n",
    "        \n",
    "        if (min(x1, x2) <= intersection_point.x <= max(x1, x2) and\n",
    "            min(y1, y2) <= intersection_point.y <= max(y1, y2) and\n",
    "            min(x3, x4) <= intersection_point.x <= max(x3, x4) and\n",
    "            min(y3, y4) <= intersection_point.y <= max(y3, y4)):\n",
    "            return intersection_point\n",
    "            \n",
    "    return None\n",
    "\n",
    "def find_intersection_points(barrier_array, car_x, car_y, end_x, end_y):\n",
    "    intersection_points = []\n",
    "    for i, barrier in enumerate(barrier_array):\n",
    "        intersection_point = intersection(barrier[0], barrier[1], barrier[2], barrier[3], car_x, car_y, end_x, end_y)\n",
    "        if intersection_point:\n",
    "            intersection_points.append(intersection_point)\n",
    "    return intersection_points\n",
    "\n",
    "def find_distance_to_intersection(intersection_points, car_x, car_y):\n",
    "    closest_distance = float('inf')\n",
    "    closest_intersection = None\n",
    "\n",
    "    for point in intersection_points:\n",
    "        x1, y1 = point\n",
    "        # Рассчитываем расстояние от автомобиля до точки пересечения\n",
    "        \n",
    "        dist = math.sqrt((x1 - car_x)**2 + (y1 - car_y)**2)\n",
    "\n",
    "        if dist < closest_distance:\n",
    "            closest_distance = dist\n",
    "            closest_intersection = point\n",
    "\n",
    "    return closest_distance, closest_intersection\n",
    "\n",
    "class MyCarEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = spaces.Discrete(3)        \n",
    "\n",
    "        self.WHITE = (255, 255, 255)\n",
    "        self.BLACK = (0, 0, 0)\n",
    "        self.GREEN = (0, 255, 0)\n",
    "        self.RED = (255, 0, 0)\n",
    "\n",
    "        self.barrier_array = np.array([[203, 270, 291, 151], [291, 151, 485, 181], [485, 181, 535, 349], [535, 349, 612, 367], [612, 367, 762, 226], \n",
    "                                               [762, 226, 962, 280], [962, 280, 980, 469], [980, 469, 934, 564], [934, 564, 739, 647], [739, 647, 641, 612],\n",
    "                                                 [641, 612, 598, 455], [598, 455, 515, 432], [515, 432, 403, 528], [403, 528, 240, 501], [240, 501, 186, 390], \n",
    "                                                 [186, 390, 205, 264], [327, 238, 279, 335], [279, 335, 297, 405], [297, 405, 391, 424], [391, 424, 449, 368], \n",
    "                                                 [449, 368, 428, 274], [428, 274, 326, 239], [670, 421, 716, 550], [716, 550, 848, 504], [848, 504, 885, 438], \n",
    "                                                 [885, 438, 879, 350], [879, 350, 784, 312], [784, 312, 670, 421]])\n",
    "\n",
    "        self.reward_lines_array = np.array([[428, 274, 486, 181], [328, 239, 290, 153], [278, 333, 203, 267], [280, 337, 186, 391], [296, 406, 239, 500], \n",
    "                                            [390, 426, 402, 527], [449, 367, 517, 433], [449, 368, 537, 348], [599, 456, 669, 421], [669, 421, 611, 367], \n",
    "                                            [784, 312, 762, 228], [879, 350, 962, 280], [886, 438, 982, 471], [846, 506, 933, 563], [716, 549, 739, 645], [717, 550, 640, 611]])\n",
    "\n",
    "        self.start_x = 560\n",
    "        self.start_y = 404\n",
    "        self.start_angle = 190\n",
    "\n",
    "        self.car_x = self.start_x \n",
    "        self.car_y = self.start_y\n",
    "        self.car_angle = self.start_angle\n",
    "        self.speed = 15\n",
    "        self.rotation_speed = 10\n",
    "\n",
    "        self.num_rays = 12\n",
    "        self.ray_length = 1000\n",
    "\n",
    "        \n",
    "        self.WINDOW_WIDTH, self.WINDOW_HEIGHT = 1268, 840\n",
    "\n",
    "        self.car_color = self.BLACK\n",
    "        self.car_width = 20\n",
    "        self.car_height = 20\n",
    "\n",
    "        self.state = np.ones(self.num_rays, dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(self.num_rays,))\n",
    "\n",
    "        self.reward = 0\n",
    "        self.car_surface = pygame.Surface((self.car_width, self.car_height), pygame.SRCALPHA)\n",
    "        \n",
    "        self.distance_array_for_fit = np.ones((1, self.num_rays), dtype=np.float32)\n",
    "\n",
    "        self.reward_last = []\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.car_x = self.start_x \n",
    "        self.car_y = self.start_y\n",
    "        self.car_angle = self.start_angle\n",
    "        self.num_steps = 0\n",
    "        self.reward = 0\n",
    "        self.reward_last = []\n",
    "        self.reward_lines_array = np.array([[428, 274, 486, 181], [328, 239, 290, 153], [278, 333, 203, 267], [280, 337, 186, 391], [296, 406, 239, 500], \n",
    "                                [390, 426, 402, 527], [449, 367, 517, 433], [449, 368, 537, 348], [599, 456, 669, 421], [669, 421, 611, 367], \n",
    "                                [784, 312, 762, 228], [879, 350, 962, 280], [886, 438, 982, 471], [846, 506, 933, 563], [716, 549, 739, 645], [717, 550, 640, 611]])\n",
    "        \n",
    "        self.barrier_array = np.array([[203, 270, 291, 151], [291, 151, 485, 181], [485, 181, 535, 349], [535, 349, 612, 367], [612, 367, 762, 226], \n",
    "                                        [762, 226, 962, 280], [962, 280, 980, 469], [980, 469, 934, 564], [934, 564, 739, 647], [739, 647, 641, 612],\n",
    "                                        [641, 612, 598, 455], [598, 455, 515, 432], [515, 432, 403, 528], [403, 528, 240, 501], [240, 501, 186, 390], \n",
    "                                        [186, 390, 205, 264], [327, 238, 279, 335], [279, 335, 297, 405], [297, 405, 391, 424], [391, 424, 449, 368], \n",
    "                                        [449, 368, 428, 274], [428, 274, 326, 239], [670, 421, 716, 550], [716, 550, 848, 504], [848, 504, 885, 438], \n",
    "                                        [885, 438, 879, 350], [879, 350, 784, 312], [784, 312, 670, 421]])\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):   \n",
    "        done = False\n",
    "\n",
    "        self.reward = 0\n",
    "        self.num_steps += 1\n",
    "        if action==0:\n",
    "            self.car_angle += self.rotation_speed  \n",
    "        elif action==2:\n",
    "            self.car_angle -= self.rotation_speed \n",
    "\n",
    "        if len(self.reward_lines_array)>1:\n",
    "            for index, line in enumerate(self.reward_lines_array):\n",
    "\n",
    "                x1, y1, x2, y2 = line\n",
    "                if  ((intersection(x1, y1, x2, y2, self.car_x-10, self.car_y-10, self.car_x+10, self.car_y+10)) or \n",
    "                                                    (intersection(x1, y1, x2, y2, self.car_x-10, self.car_y+10, self.car_x+10, self.car_y-10))):\n",
    "                    if np.any(self.reward_last):\n",
    "                        if not np.array_equal(np.array(line), self.reward_last):\n",
    "                            self.reward = 1\n",
    "                            self.barrier_array = np.append(self.barrier_array, self.reward_lines_array[np.all(self.reward_lines_array == self.reward_last, axis=1)], axis=0)\n",
    "                            self.reward_lines_array = self.reward_lines_array[~np.all(self.reward_lines_array == self.reward_last, axis=1)]\n",
    "                    else:\n",
    "                        self.reward = 1\n",
    "                    self.reward_last = np.array(line)\n",
    "                    \n",
    "\n",
    "        else:\n",
    "            self.reward_lines_array = np.array([[428, 274, 486, 181], [328, 239, 290, 153], [278, 333, 203, 267], [280, 337, 186, 391], [296, 406, 239, 500], \n",
    "                                            [390, 426, 402, 527], [449, 367, 517, 433], [449, 368, 537, 348], [599, 456, 669, 421], [669, 421, 611, 367], \n",
    "                                            [784, 312, 762, 228], [879, 350, 962, 280], [886, 438, 982, 471], [846, 506, 933, 563], [716, 549, 739, 645], [717, 550, 640, 611]])\n",
    "            \n",
    "            self.barrier_array = np.array([[203, 270, 291, 151], [291, 151, 485, 181], [485, 181, 535, 349], [535, 349, 612, 367], [612, 367, 762, 226], \n",
    "                                            [762, 226, 962, 280], [962, 280, 980, 469], [980, 469, 934, 564], [934, 564, 739, 647], [739, 647, 641, 612],\n",
    "                                            [641, 612, 598, 455], [598, 455, 515, 432], [515, 432, 403, 528], [403, 528, 240, 501], [240, 501, 186, 390], \n",
    "                                            [186, 390, 205, 264], [327, 238, 279, 335], [279, 335, 297, 405], [297, 405, 391, 424], [391, 424, 449, 368], \n",
    "                                            [449, 368, 428, 274], [428, 274, 326, 239], [670, 421, 716, 550], [716, 550, 848, 504], [848, 504, 885, 438], \n",
    "                                            [885, 438, 879, 350], [879, 350, 784, 312], [784, 312, 670, 421]])\n",
    "\n",
    "        for line in self.barrier_array:\n",
    "            x1, y1, x2, y2 = line\n",
    "            if (intersection(x1, y1, x2, y2, self.car_x-10, self.car_y-10, self.car_x+10, self.car_y+10)) or (intersection(x1, y1, x2, y2, self.car_x-10, self.car_y+10, self.car_x+10, self.car_y-10)):\n",
    "                self.reward += -5\n",
    "                done = True\n",
    "\n",
    "        radians = math.radians(self.car_angle)\n",
    "        position = Vector2(self.car_x, self.car_y)\n",
    "        direction = Vector2(math.cos(radians), -math.sin(radians))\n",
    "        position += direction * self.speed\n",
    "        self.car_x, self.car_y = position.x, position.y\n",
    "\n",
    "        end_xy = np.zeros((self.num_rays, 2))\n",
    "\n",
    "\n",
    "        for i in range(self.num_rays):\n",
    "            for j in range(2):\n",
    "                if j==1:\n",
    "                    end_xy[i][j] = self.car_y - self.ray_length * math.sin(math.radians( self.car_angle+i*360/ self.num_rays))\n",
    "                else:\n",
    "                    end_xy[i][j] = self.car_x + self.ray_length * math.cos(math.radians( self.car_angle+i*360/ self.num_rays))\n",
    "\n",
    "        self.line_intersection_xy = []\n",
    "        for end_x,end_y in end_xy:\n",
    "            self.line_intersection_xy.append(find_intersection_points( self.barrier_array, self.car_x, self.car_y, end_x, end_y))\n",
    "\n",
    "        distance_line_array = []\n",
    "        for i in range(len(self.line_intersection_xy)):\n",
    "            distance, closest_intersection = find_distance_to_intersection(self.line_intersection_xy[i], self.car_x, self.car_y)\n",
    "            if closest_intersection is not None and not math.isinf(distance):\n",
    "                distance_line_array.append(distance)\n",
    "            else:\n",
    "                distance_line_array.append(1000)\n",
    "                # self.reward = -10\n",
    "                # done = True\n",
    "        \n",
    "        # self.car_xy = np.array([int(self.car_x), int(self.car_y)])\n",
    "\n",
    "        # self.state = np.concatenate([self.car_xy, distance_line_array])\n",
    "        distance_line_array = np.array(distance_line_array)\n",
    "        self.distance_array_for_fit = np.vstack((distance_line_array, self.distance_array_for_fit))[:500]\n",
    "   \n",
    "\n",
    "        scaler.fit(self.distance_array_for_fit)\n",
    "        scaled_distance_line_array = scaler.transform(distance_line_array.reshape(1, -1))\n",
    "\n",
    "        self.state = scaled_distance_line_array.flatten()\n",
    "        \n",
    "        return self.state, self.reward, done\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "        pygame.draw.circle(self.car_surface, self.car_color, (20,20), self.car_width)\n",
    "\n",
    "        clock = pygame.time.Clock()\n",
    "        self.screen.fill(self.WHITE) \n",
    "\n",
    "        pygame.draw.line(self.screen, self.BLACK, (self.car_x-10, self.car_y-10), (self.car_x+10, self.car_y+10), 5)\n",
    "        pygame.draw.line(self.screen, self.BLACK, (self.car_x-10, self.car_y+10), (self.car_x+10, self.car_y-10), 5)\n",
    "        for line in self.barrier_array:\n",
    "            x1, y1, x2, y2 = line\n",
    "            pygame.draw.line(self.screen, self.BLACK, (x1, y1), (x2, y2))\n",
    "\n",
    "\n",
    "        for line in self.reward_lines_array:\n",
    "            x1, y1, x2, y2 = line\n",
    "            pygame.draw.line(self.screen, self.GREEN, (x1, y1), (x2, y2))\n",
    "\n",
    "        pygame.draw.circle(self.screen, self.BLACK, (self.car_x, self.car_y), (10))\n",
    "\n",
    "        for i, intersection_points in enumerate(self.line_intersection_xy):\n",
    "            closest_distance, closest_intersection = find_distance_to_intersection(intersection_points, self.car_x, self.car_y)\n",
    "            if closest_intersection:\n",
    "                if i == 0:\n",
    "                    pygame.draw.line(self.screen, self.BLACK, (self.car_x, self.car_y), (int(closest_intersection[0]), int(closest_intersection[1])), 5)\n",
    "                else:\n",
    "                    pygame.draw.line(self.screen, self.BLACK, (self.car_x, self.car_y), (int(closest_intersection[0]), int(closest_intersection[1])), 1)\n",
    "                pygame.draw.circle(self.screen, self.RED, (int(closest_intersection[0]), int(closest_intersection[1])), 5)\n",
    "        pygame.display.flip()\n",
    "\n",
    "\n",
    "        keys = pygame.key.get_pressed()\n",
    "        if keys[pygame.K_LEFT]:\n",
    "            self.car_angle += self.rotation_speed  \n",
    "        if keys[pygame.K_RIGHT]:\n",
    "            self.car_angle -= self.rotation_speed  \n",
    "        if keys[pygame.K_ESCAPE]:\n",
    "            pygame.quit()\n",
    "        \n",
    "        clock.tick(120)\n",
    "\n",
    "env = MyCarEnv()\n",
    "\n",
    "\n",
    "\n",
    "# Определение архитектуры нейронной сети для политики\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, output_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1).to(device)\n",
    "        # self.fc2 = nn.Linear(hidden_size1, hidden_size2).to(device)\n",
    "        # self.fc3 = nn.Linear(hidden_size2, hidden_size3).to(device)\n",
    "        self.fc4 = nn.Linear(hidden_size1, output_size).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x)).to(device)\n",
    "        x = torch.softmax(self.fc4(x), dim=-1).to(device)  # Функция softmax для получения вероятностей действий\n",
    "        return x\n",
    "\n",
    "class PolicyNetwork1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(PolicyNetwork1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1).to(device)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2).to(device)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x)).to(device)\n",
    "        x = torch.relu(self.fc2(x)).to(device)\n",
    "        x = torch.softmax(self.fc3(x), dim=-1).to(device)  # Функция softmax для получения вероятностей действий\n",
    "        return x\n",
    "    \n",
    "class PolicyNetwork2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
    "        super(PolicyNetwork2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1).to(device)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2).to(device)\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3).to(device)\n",
    "        self.fc4 = nn.Linear(hidden_size3, output_size).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x)).to(device)\n",
    "        x = torch.relu(self.fc2(x)).to(device)\n",
    "        x = torch.relu(self.fc3(x)).to(device)\n",
    "        x = torch.softmax(self.fc4(x), dim=-1).to(device)  # Функция softmax для получения вероятностей действий\n",
    "        return x\n",
    "\n",
    "\n",
    "# Функция для сэмплирования действия на основе политики\n",
    "def select_action(policy_net, state):\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    action_probs = policy_net(state)\n",
    "    action = np.random.choice(len(action_probs.detach().numpy()), p=action_probs.detach().numpy())\n",
    "    return action\n",
    "\n",
    "\n",
    "# Обучение агента\n",
    "def train(policy_net, optimizer, episodes, max_reward, hidden_size1, hidden_size2, hidden_size3, learning_rate):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_states = []\n",
    "        episode_actions = []\n",
    "        episode_rewards = []\n",
    "        steps = 0\n",
    "        while True:\n",
    "            action = select_action(policy_net, state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            steps += 1\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            episode_states.append(state)\n",
    "            episode_actions.append(action)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "           \n",
    "        \n",
    "            state = next_state\n",
    "            if steps >= 1000000:\n",
    "                torch.save(policy_net, f'net/tarakan1_{max_reward+5}_{hidden_size1}_{hidden_size2}_{hidden_size3}_{learning_rate}.pth')\n",
    "                print(f\"Tarakan1 Ultra Episodes {episode}:{hidden_size1}_{hidden_size2}_{hidden_size3}_{learning_rate}  SAVED!\")\n",
    "                done = True\n",
    "            if done:\n",
    "                break\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in episode_rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "\n",
    "        policy_loss = []\n",
    "        for i in range(len(episode_states)):\n",
    "            action = episode_actions[i]\n",
    "            G = returns[i]\n",
    "            state = episode_states[i]\n",
    "            action_prob = policy_net(torch.tensor(state, dtype=torch.float32))[action]\n",
    "            policy_loss.append(-torch.log(action_prob) * G)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        if sum(episode_rewards) > max_reward:\n",
    "            max_reward = sum(episode_rewards)\n",
    "            torch.save(policy_net, f'net/tarakan1_{max_reward+5}_{hidden_size1}_{hidden_size2}_{hidden_size3}_{learning_rate}.pth')\n",
    "            print(f\"Episode {episode}: Max Reward = {max_reward+5} SAVED!\")\n",
    "        print(f\"Episode {episode}: Total Reward = {sum(episode_rewards)+5} Max Reward = {max_reward+5}, hidden_size1 = {hidden_size1}, hidden_size2 = {hidden_size2}, hidden_size3 = {hidden_size3}, леарнинг рейт = {learning_rate}\")\n",
    "    return max_reward\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(policy_net, num_episodes=10):\n",
    "    total_reward = -10\n",
    "    for episode in range(num_episodes):\n",
    "        \n",
    "        state = env.reset()\n",
    "\n",
    "        while True:\n",
    "            action = select_action(policy_net, state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    env.close()\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "render = False\n",
    "\n",
    "if render:\n",
    "    env.screen = pygame.display.set_mode((env.WINDOW_WIDTH, env.WINDOW_HEIGHT))\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "# Обучение агента\n",
    "episodes = 1000\n",
    "\n",
    "\n",
    "results = []\n",
    "max_reward = -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(22, 100):\n",
    "    input_size = env.num_rays\n",
    "    output_size = 3\n",
    "    hidden_size1 = i\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    policy_net = PolicyNetwork(input_size, hidden_size1, output_size).to(device)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "    max_reward = train(policy_net, optimizer, episodes, max_reward, hidden_size1, 0, 0, learning_rate)\n",
    "\n",
    "    restest = (f'1 скрытый слой, hidden_size1 = {hidden_size1}, леарнинг рейт = {learning_rate}, счет = {test(policy_net, num_episodes=10)}')\n",
    "    print(restest)\n",
    "    results.append(restest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes=10000\n",
    "for i in range(1, 100):\n",
    "    input_size = env.num_rays\n",
    "    output_size = 3\n",
    "    hidden_size1 = i\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    policy_net = PolicyNetwork(input_size, hidden_size1, output_size).to(device)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "    max_reward = train(policy_net, optimizer, episodes, max_reward, hidden_size1, 0, 0, learning_rate)\n",
    "\n",
    "    restest = (f'1 скрытый слой, hidden_size1 = {hidden_size1}, леарнинг рейт = {learning_rate}, счет = {test(policy_net, num_episodes=10)}')\n",
    "    print(restest)\n",
    "    results.append(restest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 100):\n",
    "    for a in range(1, 100):\n",
    "        input_size = env.num_rays\n",
    "        output_size = 3\n",
    "        hidden_size1 = i\n",
    "        hidden_size2 = a\n",
    "        learning_rate = 0.001\n",
    "        \n",
    "        policy_net = PolicyNetwork1(input_size, hidden_size1, hidden_size2, output_size).to(device)\n",
    "        optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "        max_reward = train(policy_net, optimizer, episodes, max_reward, hidden_size1, hidden_size2, 0, learning_rate)\n",
    "        restest = (f'2 скрытых слоя, hidden_size1 = {hidden_size1}, hidden_size2 = {hidden_size2},леарнинг рейт = {learning_rate}, счет = {test(policy_net, num_episodes=10)}')\n",
    "        print(restest)\n",
    "        results.append(restest)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_size = env.num_rays\n",
    "output_size = 3\n",
    "hidden_size1 = 35\n",
    "learning_rate = 0.01\n",
    "\n",
    "policy_net = PolicyNetwork(input_size, hidden_size1, output_size).to(device)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "max_reward = train(policy_net, optimizer, episodes, max_reward, hidden_size1, 0, 0, learning_rate)\n",
    "\n",
    "restest = (f'1 скрытый слой, hidden_size1 = {hidden_size1}, леарнинг рейт = {learning_rate}, счет = {test(policy_net, num_episodes=10)}')\n",
    "print(restest)\n",
    "results.append(restest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m env\u001b[38;5;241m.\u001b[39mscreen \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mset_mode((env\u001b[38;5;241m.\u001b[39mWINDOW_WIDTH, env\u001b[38;5;241m.\u001b[39mWINDOW_HEIGHT))\n\u001b[0;32m      3\u001b[0m policy_net \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnet/tarakan1/6_0_0_0.0001_442.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[2], line 391\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(policy_net, num_episodes)\u001b[0m\n\u001b[0;32m    388\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m render:\n\u001b[1;32m--> 391\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Total Reward = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 264\u001b[0m, in \u001b[0;36mMyCarEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys[pygame\u001b[38;5;241m.\u001b[39mK_ESCAPE]:\n\u001b[0;32m    262\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mquit()\n\u001b[1;32m--> 264\u001b[0m \u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "render = True\n",
    "env.screen = pygame.display.set_mode((env.WINDOW_WIDTH, env.WINDOW_HEIGHT))\n",
    "policy_net = torch.load(f'net/tarakan1/6_0_0_0.0001_442.pth')\n",
    "print(test(policy_net, num_episodes=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net, f'net/tarakan_ultra.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
